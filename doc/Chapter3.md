# 3章 教師なし学習

------------------------------------------------------------------------------

教師なし学習の分類

+ 教師なし変換(Unsupervised transformation): 元のデータを人や機械学習アルゴリズムがわかりやすいデータへ自動的に変換するアルゴリズム．特徴量の次元削減など．

+ クラスタリングアルゴリズム(Clustering algorithms): 似たようなデータをグループ分けするアルゴリズム．

教師なし学習は正解データ(ラベル)が与えられないため，モデルの評価が難しい．
自動システムの一部としてではなく，データサイエンティストがデータを理解するために探索的に用いる場合がある．教師あり学習の前処理としても使う．

------------------------------------------------------------------------------

## スケール変換

------------------------------------------------------------------------------

教師データで作成(適用)したスケーラを用いて，訓練データもスケールする必要がある．

スケール変換による前処理がモデルに与える影響は大きく，SVCの例だと，MinMaxScalerを通すだけでスコアが0.63から0.97まで向上した．

### StandardScaler

各特徴量が平均0, 分散1になるようにスケーリングする．
最大値最小値をある値の範囲に入れようとしない．

### RobustScaler
ここの特徴量をある値の範囲に収めようとする．
平均・分散の代わりに，中央値と四分位数を用いる．
四分位数の外にある外れ値を除去してくれる．(他のスケーリングで外れ値は問題になる)

### MinMaxScaler

```py
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

X_train_scaled = scaler.fit_transform(X_train)
# X_train_scaled = scaler.fit(X_train).transform(X_train) # 上記と同等
```

データの最小値が0，最大値が1になるよう，0から1の範囲にすべてのデータが入るようにスケールする．

### Normalizer
各データポイントの特徴ベクトルのユークリッド長が1になるように変換する．(すべてのデータポイントを半径1の超球面へ射影する)
言い換えると `x = [x1, x2, ... xn]`のとき，`sqrt( x1^2 + x2^2 + ... + xn^2 ) = 1` となるように，`x`を変換する.

特徴ベクトルの長さではなく，方向のみに関心がある場合に用いられる．

------------------------------------------------------------------------------

## 次元削減，特徴量抽出，多様体学習

------------------------------------------------------------------------------

### 主成分分析(PCA: Principal component analysis)
```py
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
```

![pca](img/pca.png)

データセットの特徴量を相互に統計的に関連しないように回転する．

+ データセットの分散が最も大きい方向を第一主成分(Component 1)とし，それと直交する方向のうち最も分散が大きい方向を第二主成分(Component 2)とする．主成分は互いに直交している，互いに相関はない．
+ これを繰り返して主成分を抽出する．一般に，もとの特徴量と同じだけ主成分を抽出できるが，必要なものだけ残すことで次元を削減できる．
+ こうして見つけた主成分を特徴とする(回転)．

##### パラメータ
+ `n_components`: 残したい主成分の数
+ `whiten`: Trueにすると回転だけではなく，分布が円に近くなるようにスケール変換もしてくれる．

#### 次元削減(可視化)
PCAは次元削減の他に，高次元データセットの可視化にも使える．主成分2つでプロットした例．

![pca_cancer](img/pca_cancer.png)

今回のデータではたまたま，線形にある程度分離できそうに見えるので，線形のクラス分類アルゴリズムが使えそう・・・といった感じで当たりをつけていくのに使える．

主成分は元の特徴量の組み合わせであるため説明が難しい．

`pca.components_`で主成分ごとに，元の特徴量に対する係数を確認できる．(値が大きいほど，対応する元の特徴量の影響が大きい)

![pca_heatmap](img/pca_heatmap.png)

#### 特徴量抽出

与えられたデータの元の表現より解析に適した表現があるはず．
たとえば画像を比較する際，バイト列で比較するのは全然だめなので，画像関連には特徴量抽出が効果的．

![face_pca](img/face_pca.png)

顔画像の分類では，クラス(つまり人)ごとの訓練データが少なかったり，新しいクラスを追加しようとするとモデルを作り直したりする必要がある．
1-最近傍法クラス分類で，一番近い顔を探す．

------------------------------------------------------------------------------

### 

------------------------------------------------------------------------------

## Keywords

+ 四分位数: 大小関係によってデータセットを1:3または3:1に分割する値．
  + 第1四分位数: `大3:小1` に分割する値(下側)
  + 第3四分位数: `大1:小3` に分割する値(上側)

+ 外れ値(outliner):
+ 正規化: データ等々を一定のルール（規則）に基づいて変形すること
+ Labeled Faces in the Wild: 有名人の顔画像セット．
+ 1-最近傍法クラス分類: k=1の k-最近傍法クラス分類