## 2章 教師あり学習
+ クラス分類: ラベルの予測．選択肢の中からクラスラベルを予測すること
+ 回帰: 連続値の予測．量を予測する．
+ 過剰適合(overfitting): 情報量に比べて過度に複雑なモデルを作ってしまうこと
+ 適合不足(underfitting): 単純すぎるモデルを選択してしまうこと
+ 特徴量エンジニアリング(feature engineering): 測定結果としての特徴量間の積(交互作用)から導出した新たな特徴量を含めること．
+ 学習曲線(learning curve): モデルの性能をデータセットサイズの関数として示したもの
+ 訓練セットのスコアが高く，テストセットのスコアが低い場合，過剰適合の可能性が高い．
+ 訓練セットとテストセットの精度がとても近い場合，適合不足の可能性が高い．
+ 適合不足の場合は複雑なモデルになるよう調整する
+ 過剰適合の場合はシンプルなモデルになるよう調整する
+ `COO(Coodinate)-format` : 0成分を省略したフォーマット．メモリ効率が良い．
+ 2値データ: 特徴量ごとに非ゼロをカウントした疎なデータ
+ カウントデータ: (文中の単語数などの)整数カウントした疎なデータ
+ アンサンブル法: 複数の機械学習モデルを組み合わせること．ランダムフォレストと勾配ブースティング決定木がよく知られている．

------------------------------------------------------------------------------
### k-最近傍法(k-NN) : `クラス分類`
テストデータに，最も近い訓練データn点で多数決する．
nが小さいほど，出来上がるモデルは複雑であり，
nがサンプル数と等しい場合が最も単純であると考える．
一般に，モデルが複雑すぎても単純すぎても性能が低下する．

```py
from sklearn.neighbors import KNeighborsClassifier
KNeighborsClassifier(n_neighbors=n).fit(X_train, y_train).score(X_test, y_test)
```

### k-近傍回帰 : `回帰`
テストデータに，最も近い訓練データn点の平均値を予測値とする．

```py
from sklearn.neighbors.regression import KNeighborsRegressor
KNeighborsRegressor(n_neighbors=3).fit(X_train, y_train).score(X_test, y_test)
# 回帰問題なので R^2スコアを返す
```

### KNeighborsの特徴
+ 主なパラメータ
  + 近傍点数: 通常は3から5程度でOK
  + 距離尺度: 通常はユークリッド距離

+ 点数が増えた場合，モデル構築は高速(点を追加するだけなので)だが予測(点の走査)は遅くなる．
+ 数百を超える特徴量を持つデータではうまく機能せず，疎なデータセット(ほとんどの特徴量が0)では特に性能が悪いため，実際はほとんど使われてない．

------------------------------------------------------------------------------
### 線形モデル
入力特徴量の重み付き線形和が予測されるレスポンスyになる．各重みWとバイアスbがモデルを構成する．

訓練データのサンプル数より特徴量のほうが多い場合，どのような`y`であっても完全にデータセットの線形関数としてモデル化できる．(それらが線形独立であると仮定？)
これによって過剰適合の危険がある．

特徴量が少ない場合，線形モデルはあまり適さない．(線形にしか分離できないので，どうやっても分離できないケースが多くなる)
特徴量が多く高次元となる場合は線形モデルによる分類は非常に強力．
そのため，特徴量が多い場合での過剰適合を回避する方法が重要になってくる．

線形モデルによる回帰では，まずはRidge回帰を試してみて，特徴量が沢山あるが一部のみが重要であることがわかっている場合はLasso回帰を検討するのが良い．

回帰およびクラス分類のいずれにおいても，モデルの係数の重みは予測や分類の結果への影響度を示している．
しかしながら，`C`や`alpha`によってこれら係数の正負が入れ替わったりする場合もあるため，眉唾で解釈しなければならない．



##### サンプルが多い(10万,100万)場合
+ LogisticRegressionやRidgeの`solver='sag'`オプションを使うとデフォルトより高速になる場合がある．
+  SGDClassifier: `クラス分類` や SGDRegressor:`回帰` の仕様を検討する．

##### 主なパラメータと特徴
+ 正則化(ペナルティ)
  + `L1`: 一部の特徴量のみを使う．使用しない特徴量の重みは完全に0になる．一部の特徴量が重要であることがわかっている場合に使うと良い．
  + `L2`: 一部の特徴量を主に使う．重みは0にはならない．とくに理由がなければこちらを使うのが良い．
+ 正則化パラメータ: `alpha`が大きい場合や`C`が小さい場合，モデルが単純になる．
+ ロス関数: 訓練データの適合度を計る尺度．基本はユークリッド距離の最小二乗誤差

訓練が非常に高速で予測も高速．大きなデータセットで使われることが多いが，これは他のモデルでは学習できないため．
予測手法を理解しやすいが，特徴量間の相関がわかりにくく，係数の意味を理解しにくい．
特徴量の数がサンプル個数より多い場合に良い性能が出やすい．


#### 線形回帰(最小二乗法) : `回帰`
訓練データに対してターゲットとの平均二乗誤差(mean squared error, 誤差の二乗の平均)が最小になるようW(重み，係数)とb(切片)を求めたもの．

線形回帰ではテストデータが少ないとほとんどなにも学習できない．
一般的にRidge回帰やLasso回帰のほうが汎化性能が良いため，線形回帰を使用する機会はほぼ無い．

```py
from sklearn.linear_model import LinearRegression
LinearRegression().fit(X_train, y_train).score(X_test, y_test)
```

線形回帰はパラメータを必用とせずお手軽だが，逆にモデルの複雑さをコントロール出来ないことを意味する．
つまり，過剰適合を抑制する手段がない．(そのためにRidge回帰やLasso回帰を使う)
特に線形モデルでは，訓練データの特徴量が多く十分な訓練データ数を用意できない場合，過剰適合が起こりやすいため注意が必要．

#### リッジ回帰(Ridge) : `回帰`
平均二乗誤差を最小化するのは線形回帰と同様．
Wを正則化(regularization)して，ここの特徴量が出力に与える影響をなるべく小さくすることで，モデルが単純となり，過剰適合を防ぐ．
汎化性能面では，*一般に線形回帰よりリッジ回帰を使ったほうが良い* ．

```py
from sklearn.linear_model import Ridge
Ridge(alpha=1.0).fit(X_train, y_train).score(X_test, y_test)
```

モデルの簡潔さ(0に近い係数の数)は，パラメータ`alpha`で制御できる．(デフォルトは1.0)
`alpha`が大きいほど，モデルは簡潔となるため，汎化にはそちらのほうがよい場合がある．
これらはデータに依存するため，パラメータを適切に設定する必要がある．
なお，十分な訓練データがある場合は，正則化はあまり重要ではなく，線形回帰と同じ性能を示す．

リッジ回帰の正則化はL2ノルム(各次元の重みを2乗した和=Wのユークリッド長)に対してペナルティを与える．(L2正則化)

#### Lasso : `回帰`
Lassoの正則化はL1ノルム(重みの絶対値の和)にペナルティを与える．(L1正則化)
Ridgeとの違いは，いくつかの重みは完全に0になるため，特徴量が選択されていると考えても良い．
(例えば，特徴量が104あって，そのうち33個だけを使うなど)

```py
from sklearn.linear_model import Lasso
Lasso().fit(X_train, y_train).score(X_test, y_test)
```

同様に`alpha`を減らすと複雑なモデルを構築して適合度合いを上げることができる．
`alpha`を調整することで，使う特徴量を制限しつつ，Ridgeと同程度の性能を出すことができる．


------------------------------------------------------------------------------
#### クラス分類のための線形モデル : `クラス分類`
重み付き線形和が0より大きいかどうかで分類する．

+ ロジスティック回帰(logistic regression) : `クラス分類`
  ```py
  from sklearn.linear_model import LogisticRegression
  LogisticRegression(C=1.0, penalty='l2').fit(X_train, y_train).score(X_test, y_test)
  ```
b
+ 線形サポートベクタマシン(linear SVM) : `クラス分類`
  ```py
  from sklearn.svm import LinearSVC
  LinearSVC(C=1.0, penalty='l2').fit(X_train, y_train).score(X_test, y_test)
  ```

回帰という名前がついているが，クラス分類のアルゴリズムである．
これらはデフォルトでL2正則化を行う(特徴量削減をしない)が，L1正則化を行うこともできる．(`penalty`)
正則化の度合いはパラメータ`C`で制御され，Cが大きいほど正則化が弱くなり，複雑なモデルになる．RidgeやLassoの`alpha`とは逆．

正則化が弱くなると訓練データに対するスコアは上がるが，過剰適合しやすくなる．

#### 線形モデルによる多クラス分類
1対その他(one-vs.-rest)アプローチ: クラスごとに2クラス分類器(ベクトルと切片)を用意する．

どの分類器でもそれ以外に分類される領域については，クラス分類式の値が一番大きいクラス(つまりその点に最も近い線を持つクラス)に分類される．


------------------------------------------------------------------------------
### ナイーブベイズクラス分類器:`クラス分類`
線形モデルに比べて訓練が高速だが性能は若干悪い．
クラスごとの特徴量を計算する．また，特徴量の特性に応じて使い分ける．

+ `GaussianNB`: 連続値データ．訓練では特徴量の平均と標準偏差を格納し，予測では最もよく適合するクラスが採用される．高次元データ向き．
+ `BernoulliNB`: 2値データ．特徴量ごとに非ゼロをカウントする．疎なデータ向き．
+ `MultinomialNB`: カウントデータ(文中の単語数などの整数カウント)など疎なデータ向き．

##### 主なパラメータ
+ `MultinomialNB` と `BernoulliNB`について
  + `alpha`: 大きいほどシンプルなモデルになる．(全ての特徴量に対して，仮想的なデータポイントが`alpha`に応じて追加される)．この`alpha`によってアルゴリズムの性能が大きく変化することはない．


------------------------------------------------------------------------------
### 決定木(DecisionTree)

```py
from sklearn.tree import DecisionTreeClassifier
DecisionTreeClassifier().fit(X_train, y_train).score(X_test, y_test)
```

2分木，節が述語(機械学習では`テスト`というらしい)で葉がクラス．
クラス分類と回帰の両方で使える．(クラス分類決定木, 回帰決定木)

過剰適合しやすく汎化性能が低い傾向があるため，ほとんどの場合，アンサンブル法によるランダムフォレストが用いられる．
上記は決定木に比べて予測の過程が複雑であるため，専門家でない人に可視化して見せたい場合，決定木を使用するのも良いかもしれない．

決定木を構築する際，目的変数(`y`)に対して情報量の多いテストから順にルートから配置する．
分割後の領域も再帰的に分割し，領域に単一クラス(または回帰値)のみが含まれるまで分割する．
単一データポイントのみ含むような決定木の葉を`純粋`という．

クラス分類の場合は，ルートから順にテストで分岐して葉までたどれば良い．

回帰決定木による予測では，同様に葉まで辿って，その葉に属する訓練データの平均値を予測値とする．
そのため，**訓練データの範囲を超えた予測(外挿(extrapolate))をすることはできない**ことに注意.

複雑さを抑制する方法として以下の2つがある．
+ 事前枝刈り: 構築中に生成を止める(木の深さを制限(`max_depth=4`)，葉の大きさを制限)
+ 事後枝刈り: 木を作ってから情報の少ないノードを削除する

scikit-learnでは事前枝刈りしか実装されていない．


木をgraphvizで表示する
```py
from sklearn.tree import export_graphviz
export_graphviz(tree, out_file="tree.dot", class_names=["malignant", "benign"], feature_names=cancer.feature_names, impurity=False, filled=True)

# pip install grahviz
# brew install graphviz
import graphviz
with open(tree_file) as f:
  dot_graph = f.read()
graphviz.Source(dot_graph)
```

決定木を可視化すると予測過程を説明しやすい．


##### 特徴量の重要度(feature importance)
```py
tree.feature_importances_
```
このモデル(木)において，合計が1になるようにした，各特徴量に対する重要度の重み．分類にどの程度寄与しているかを示す．
値が小さいからといって，その特徴量が重要ではない，というわけではない．単にこのモデルで採用されなかっただけ．また，他の特徴量にエンコードされている場合もあるので注意．

##### 主なパラメータと特徴
+ 事前枝刈りパラメータ．以下のいずれかで制限すれば十分．
  + `max_depth`: 木の深さ
  + `max_leaf_nodes`:
  + `min_samples_leaf`

+ Good: モデルの可視化が容易であり，特徴量の正規化や標準化が不要で2値と連続値が混在していてもOK
+ Bad: 過剰適合しやすく汎化性能も低い傾向がある．


------------------------------------------------------------------------------
### ランダムフォレスト
```py
from sklearn.ensemble import RandomForestClassifier
RandomForestClassifier().fit(X_train, y_train).score(X_test, y_test)
```

回帰およびクラス分類において，現在最も広く使われている機械学習手法である．
それぞれ異なった方向に過剰適合した決定木を複数集めたもの．決定木の課題である一部データへの過剰適合を平均で緩和する．
決定木の構築過程(データポイント選択，テストの特徴量選択)で乱数を用いる．
決定木の本数はパラメータ`n_estimators`で決定する(実際のアプリケーションでは数百から数千とする場合が多い)
なお，各特徴量の重要度を平均することで，決定木と同様にどの特徴量の重要度を計算できる．


##### ランダムフォレストの構築
+ ランダムなデータポイント選択(ブートストラップサンプリング)
`n_samples`個のデータポイントから復元抽出(取り出して元に戻す抽出方法．同じサンプルが何度も選ばれることがある)で`n_samples`個を取り出し，訓練データとする．だいたい30%ほどが欠けて重複データになる．

+ ランダムなテストの特徴量選択
テストごとに，サイズが`max_features`となる特徴量のサブセットを選択し，その中から最適な特徴量を選択する．
つまり `max_features = n_features`とした場合，ランダム性はなくなる．
`max_features`が大きいと訓練データ対して良く適合するが，同じように過剰適合した木ができやすくなる．
`max_features`が小さいと異なる木ができやすいが，訓練データに十分に適合するには木を深くしなければならない．

多くの場合，ランダムフォレストはデフォルトのパラメータで良く機能する．


##### 予測
全ての木で予測を行い，回帰の場合は平均，クラス分類の場合は`ソフト投票`を行う．
`ソフト投票`: 木は出力ラベルに対応する確率を出力し，その確率の平均が最も高かったラベルをランダムフォレストの予測ラベルとする．


##### 主なパラメータと特徴
+ `n_estimators`: 木の数．大きければ大きいほどよい．処理時間とメモリが許す限り大きくするとよい．
+ `max_features`: ランダムに抽出るするな特徴量サブセットのサイズ．デフォルト値(クラス分類は`sqrt(n_features)`，回帰は`n_features`)で良いだろう．
+ `n_samples`: ブートストラップサンプリングのサンプル数
+ 決定木同様の事前枝刈りパラメータ

+ `n_jobs`: 木を並列処理で作る際に使用するコア数．`-1`を指定すると使えるだけ使う．

高次元で疎なカウントデータは苦手で，線形モデルのほうが適している．
訓練および予測処理は線形モデルより遅く，多くのメモリを必要とする．





------------------------------------------------------------------------------
### [`2_SupervisedLearning.py`](../src/2_SupervisedLearning.py)
+ `plt.scatter` で散布図を表示
+ `np.bincount(cancer.target)` で，値(ラベル)ごとに集計したndarrayを返す
+ 複数の線を同じ表に表示したければ，`plt.plot` を複数回コールする
+ k-最近傍法を回帰に対応させた`k-近傍回帰`がある．
+ k-近傍回帰: `from sklearn.neighbors import KNeighborsRegressor`
+ `plt.subplots` でトレリス表示できる
+ `reshape(n,m)` でデータを n行 m列にする．mかnに-1を指定すると要素数を元によしなにやってくれる
+ `np.linspace(-3, 3, 1000).reshape(-1, 1)` とかすると，1000行1列データにできる

+ `lr.coef_` : 重み W
+ `lr.intercept_` : バイアス,切片 b
+ `scikit-learn`では，訓練データから得られた属性には全て，末尾にアンダースコアをつける習慣がある．(`coef_`, `intercept_` など)
+ `LogisticRegression(penalty="l1")`として，どのようなペナルティ(ルール)で正則化するか指定できる

+ `export_graphviz` でtreeを可視化できる
+

------------------------------------------------------------------------------
